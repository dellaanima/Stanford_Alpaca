{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 세트의 수: 52002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Give three tips for staying healthy.',\n",
       " 'input': '',\n",
       " 'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 경로\n",
    "file_path = './alpaca_data.json'\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# 데이터 세트의 수 확인\n",
    "data_count = len(data)\n",
    "\n",
    "print(f\"데이터 세트의 수: {data_count}\")\n",
    "\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/dellaanima/.local/lib/python3.10/site-packages (0.28.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (2.2.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: psutil in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dellaanima/.local/lib/python3.10/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: sympy in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: fsspec in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.0)\n",
      "Requirement already satisfied: filelock in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.1.48)\n",
      "Requirement already satisfied: networkx in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.5.86)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.87)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.0.86)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.8.86)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/dellaanima/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.7.0.84)\n",
      "Requirement already satisfied: requests in /home/dellaanima/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dellaanima/.local/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dellaanima/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dellaanima/.local/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.2.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/dellaanima/.local/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'data_output.json'.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import utils\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = utils.jload(data_path)\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        self.sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "\n",
    "        self.targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(self.sources[:10], self.targets[:10], tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "    def get_sources_and_targets(self):\n",
    "        return self.sources, self.targets\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path='./alpaca_data.json')\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset, eval_dataset=None, data_collator=data_collator)\n",
    "\n",
    "# 사용 예시\n",
    "def main():\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        \"facebook/opt-6.7b\",\n",
    "        model_max_length=512,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "    )\n",
    "\n",
    "    dataset = SupervisedDataset(data_path='./alpaca_data.json', tokenizer=tokenizer)\n",
    "    sources, targets = dataset.get_sources_and_targets()\n",
    "\n",
    "\n",
    "    # sources와 targets를 사용하여 DataFrame 생성\n",
    "    data = pd.DataFrame({\n",
    "        'Source': sources,\n",
    "        'Target': targets\n",
    "    })\n",
    "\n",
    "\n",
    "    # JSON 파일로 저장\n",
    "    data.to_json('data_output.json', orient='records', lines=True)\n",
    "\n",
    "    print(\"Data saved to 'data_output.json'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
